---
title: "DataProcessing"
author: "Leo Kitchell"
date: "11/27/2021"
output: html_document
---

# Purpose
The code below creates models to predict neighborhood (census tract) level gentrification between 2010-2014 and 2015-2019 using Airbnb data.

## Installing Packages & Libraries
```{r,warnings = FALSE}
library(stratification)
library(caTools)
library(tm)
library(SnowballC)
library(readr) 
library(randomForest)
library(tidyverse)
library(glmnet)
library(caret)
library(ranger)
```


# Reading Data and Final Compilation
Data is aggregated at the airbnb listing level. Some final data collection steps are performed here because the data files were too large to push to github

## Stacking Review Data
```{r}
runtime <- list()
runtime$finalCompilation <- Sys.time()
### Stacking Review Data
path = "./InsideAirbnbData/Zipped_Data/"
file_list = list.files(path,'*.csv')
review_data <- array()
for (variable in file_list) {
  if(grepl("reviews",variable,fixed=TRUE)){
      review_data <- rbind(review_data,read.csv(paste(path,variable,sep = ""),encoding = "UTF-8"))
  } 
}
```

## Eliminating Review Data Outside of Time Window and Aggregating to Listing Level
```{r}
runtime$loadAndFilter <- Sys.time()
### Prefix "t" added representing they will be used for the text analysis portion. Alternately, "t" for tiny.
load("Compiled_Thesis_Data.RData")
tReviewData <- review_data %>% filter(!is.na(id)) #removes rows with NA listing IDs. 
tReviewData$date <- as.Date(tReviewData$date)
tReviewData <- tReviewData %>% filter(date >= as.Date("2015-01-01") & date <= as.Date("2019-12-31"))  #only keep reviews between census collection time period: 2015-2019

tReviewData <- tReviewData %>% subset(select = c(listing_id,comments)) #keep only listing_id and text data
tReviewData <- tReviewData %>% group_by(listing_id) %>% mutate(numReviews = n()) #creates a feature for the number of reviews per listing
tReviewData <- aggregate(comments ~ listing_id + numReviews, tReviewData, paste, collapse = " ") #combines all text data for a given listing into one row for that listing
runtime$loadAndFilter <- Sys.time() - runtime$loadAndFilter
```

## Merging Airbnb (Listing Level) and Census Data (Tract Level)
```{r}
tListingData <- listing_data %>% subset(select = c("id","bedrooms","price","minimum_nights","review_scores_rating",
                                                             "review_scores_location","license","tractFIPS2010"))

colnames(tListingData)[which(names(tListingData) == "price")] <- "listing_price" #renames column titled price to "listing_price". 
                                                                                 #Prevents error of repeated column name in tSparse

airbnbData <- inner_join(tListingData,tReviewData,by = c("id"="listing_id"))

tCensusData <- gentrifiableCensusDataWithDistributionsWide %>% subset(select = c("tractFIPS","NAME.2014","totalPop.2014","popGrowth","gentrificationChange"))
combinedData  <- inner_join(tCensusData,airbnbData,by = c("tractFIPS"="tractFIPS2010"))
```

## Aggregating Airbnb Data to Tract Level
```{r}
tCombinedData <- combinedData %>% subset(select = c(tractFIPS,comments))
tCombinedData <- aggregate(comments ~ tractFIPS, tCombinedData, paste,collapse = " ")

combinedData <-  combinedData %>% subset(select = -c(comments))

combinedData <- combinedData %>% group_by(tractFIPS) %>%
                                summarise(gentrificationChange = mean(gentrificationChange,na.rm = TRUE),
                                            totalPop.2014 = mean(totalPop.2014,na.rm = TRUE),
                                            popGrowth = mean(popGrowth,na.rm = TRUE),
                                            review_scores_rating = mean(review_scores_rating,na.rm = TRUE),
                                            review_scores_location = mean(review_scores_location,na.rm = TRUE),
                                            bedrooms = mean(bedrooms,na.rm = TRUE),
                                            listing_price = mean(parse_number(listing_price),na.rm = TRUE),
                                            numListings = n(),
                                            numReviews = sum(numReviews))

tractData <- inner_join(combinedData,tCombinedData, by = "tractFIPS")

rm(tReviewData,airbnbData,tCensusData,tCombinedData,combinedData)
runtime$finalCompilation <- Sys.time() - runtime$finalCompilation
```


# To Do (Not in order):
 5) Run model with TFDIF
 1) Try different values of sparsity. .95,.99,.995.   Look at the MSE for each model. Take the mean(MSE) for each model.
 2) DONE: Aggregate to the tract level. 
 3) DONE: run model with all variables so i can see the relative importance
 4) DONE: add a list of city names as stopwords. Also state names. 
 6) DONE: Investigate packages that will allow parallel processing (caret? foreach?)

## Creating Location Stopwords
```{r}
includedStates <- listing_data %>% group_by(listing_data$state) %>% summarize() %>% drop_na()
names(includedStates)<-"state_FIPS"
cities <- unlist(str_split(file_list,"listings"))
cities <- cities[substr(cities,0,1)=="_"]
cities <- substr(cities,2,nchar(cities)-4)
cities <- tolower(cities)
cities <- c(cities,"angeles","york","diego","francisco","minneapolis")

states <- read.csv("./OtherData/State_Fips_Codes.csv",colClasses = "character")
states <- inner_join(states,includedStates,by = c("State_FIPS"="state_FIPS"))
states <- tolower(unlist(states$State_Name))
locationWords <- c("north","east","south","west","northeast","northwest","southeast","southwest",states, cities)
```

## Cleaning Text Data
Takes approximately 2 hours to run.
```{r}
runtime$corpus <- Sys.time()

#sampleSize <- nrow(tractData)
#combinedDataSample <-tractData[sample(nrow(tractData), sampleSize), ]
#combinedDataSample <-tractData[1:sampleSize,]
#corpus <- Corpus(VectorSource(combinedDataSample$comments)) # REMOVE when running full thing

corpus <- Corpus(VectorSource(tractData$comments)) 
corpus <- tm_map(corpus, PlainTextDocument)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, "New") #removing for place names (e.g. New York, New Jersey)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removeWords, c("airbnb",stopwords("english"),stopwords("spanish"),locationWords))  #remove stopwords
corpus <- tm_map(corpus, stemDocument)
#save(corpus,file = "processedFullCorpus.RData")
runtime$corpus <- Sys.time() - runtime$corpus
```

## Creating Matrices of Word Frequencies
Both Bag of Words and TF IDF are created. Different sparsity thresholds are also tested
```{r}
sparsities <- c(.5,.6,.7,.8,.9,.95,.99)
frequencies <- DocumentTermMatrix(corpus)
tfidFrequencies <- DocumentTermMatrix(corpus,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))

sparseList <- list()
tSparseList <- list()
ii<-1
while (ii < length(sparsities)+1) {
  sparseList[[ii]] <- removeSparseTerms(frequencies, sparsities[ii])
  tSparseList[[ii]] <- as.data.frame(as.matrix(sparseList[[ii]]))
  colnames(tSparseList[[ii]]) <- make.names(colnames(tSparseList[[ii]]))
  ii <- ii+1
}
```

## Creating and Combining Predictor Data
```{r}
predictorData <- subset(tractData,select = c(tractFIPS,gentrificationChange,totalPop.2014,popGrowth,review_scores_rating,
                                                review_scores_location,bedrooms,listing_price,numListings))

tSparse <- cbind(predictorData[1:sampleSize,],tSparse)
tSparse <- tSparse[,!duplicated(colnames(tSparse))] #removing columns with duplicate names

# #create a very small training set to get the code working.
# index1 <- sample(seq_len(nrow(tSparse)), size = nrow(tSparse)*.05) 
# smallTSparse <- tSparse[index1, ]
# index2 <- sample(seq_len(nrow(smallTSparse)), size = nrow(smallTSparse)*.7) 
# smallTrainSparse <- smallTSparse[index2,]
# smallTestSparse <-  smallTSparse[-index2,]
# 
#sum(!complete.cases(tSparse)) #number of rows with NAs. (There are 500 total)
#incompleteCases<-(tSparse[!complete.cases(tSparse),])
#tSparse<-(tSparse[complete.cases(tSparse),]) #removing rows which contain 


trainProportion <- 0.7
speedFactor <- .01 #Value between 0 and 1. Percent of tracts to use in training and testing set. When running full model set to 1. 
speedSparseList <-list()
indexSpeedSparse <- list()

ii<-1
while (ii < length(tSparse)+1) {
  tSparseList[[ii]]     <- cbind(predictorData[1:sampleSize,],tSparseList[[ii]])                 #combine text and structured features
  tSparseList[[ii]]     <- tSparseList[[ii]][,!duplicated(colnames(tSparseList[[ii]]))]           #remove columns with duplicate names
  tSparseList[[ii]]     <- tSparseList[[ii]][complete.cases(tSparseList[[ii]]),]                  #remove rows with NAs and NaNs
  
  indexSparseTrain[ii]  <- sample(seq_len(nrow(tSparseList[[ii]])), size = nrow(tSparseList[[ii]])*trainProportion) #create a training and testing dataset
  trainSparseList[[ii]] <- tSparseList[[ii]][indexSparse[ii],]    
  testSparseList[[ii]]  <- tSparseList[[ii]][-indexSparse[ii],]
  
  indexSpeedSparse[ii]  <- sample(seq_len(nrow(tSparseList[[ii]])), size = nrow(tSparseList[[ii]])*speedFactor)   #create a subset of the full dataset to train small models on
  speedSparseList[[ii]] <- tSparseList[[indexSpeedSparse[ii]],]
  indexSpeedSparseTrain[ii]  <- sample(seq_len(nrow(speedSparseList[[ii]])), size = nrow(speedSparseList[[ii]])*trainProportion)
  speedTrainSparseList[[ii]] <-speedSparseList[[ii]][indexSpeedSparseTrain[ii],] 
  speedTestSparseList[[ii]] <- speedSparseList[[ii]][-indexSpeedSparseTrain[ii],] 

  ii<-ii+1 
}




```

## Splitting Data into Testing and Training Sets
```{r}
index <- sample(seq_len(nrow(tSparse)), size = nrow(tSparse)*.7) #splits 70% of the data into a trainng set and 30% into a test set.
trainSparse <- tSparse[index, ]
testSparse <- tSparse[-index, ]
save(list = c("trainSparse","testSparse","tSparse","tractData","corpus"), file = "preModel_12_1.RData")
```

#Multithreaded Random Forest - Ranger
```{r}
runtime$RF_Multi <- Sys.time()

testSparse2 <-   testSparse[,2:ncol(testSparse)]
trainSparse2 <- trainSparse[,2:ncol(trainSparse)]

RF_model_MultiThread3 <- caret::train(gentrificationChange ~ .,
                                      data =trainSparse2,
                                      method = "ranger",
                                      importance = "impurity",
                                      num.trees = 100)


predictRF_Multi = predict(RF_model_MultiThread3, newdata=testSparse2)

plot(varImp(RF_model_MultiThread3),top = 15)


runtime$RF_Multi <- Sys.time() - runtime$RF_Multi
```

#Multithreaded Random Forest - Ranger
```{r}
runtime$RF_Multi2 <- Sys.time()

RF_model_MultiThread3 <- caret::train(gentrificationChange ~ .,
                                      data =trainSparse2,
                                      method = "ranger",
                                      importance = "impurity",
                                      num.trees = 600)


predictRF_Multi2 = predict(RF_model_MultiThread2, newdata=testSparse2)
runtime$RF_Multi2 <- Sys.time() - runtime$RF_Multi2
```

## Running Random Forest Model
```{r}
runtime$RF <- Sys.time()
mean(tSparse$gentrificationChange,na.rm = TRUE)  #baseline accuracy??
RF_model = randomForest(gentrificationChange ~ . -tractFIPS, data=trainSparse,na.action = na.omit, ntree = 2)
save(RF_model,file = "RF_model2.RData")

predictRF = predict(RF_model, newdata=testSparse)
hist(predictRF)
varImpPlot(RF_model, n.var = 25)
RF_model$rsq

runtime$RF <- Sys.time() - runtime$RF
paste("finished at:",Sys.time())
```

$RF300
Time difference of 1.253781 mins
$RF1000
Time difference of 6.978322 mins
$RF150
Time difference of 1.977194 mins
Time difference of 1.605971 mins (ran again)

## Ridge Regression
```{r}
x <- as.matrix(subset(trainSparse, select = -gentrificationChange))
y <- subset(trainSparse, select = gentrificationChange)
ridgeReg <- cv.glmnet(x = x[],
                      y = y[,],
                      alpha = 0)
bestLambdaRidge <- ridgeReg$lambda.min
bestFitRidge <- ridgeReg$glmnet.fit
bestRidgeReg <- glmnet(x[], y[,], alpha = 0, lambda = bestLambdaRidge)

testSparse$predictedGentrification <- predict(bestRidgeReg,
                                              s = bestLambdaRidge,
                                              newx =as.matrix(subset(testSparse,
                                                                      select = -c(gentrificationChange)))[])
rssRidge <- sum((testSparse$predictedGentrification - testSparse$gentrificationChange)^2)
tssRidge <- sum((testSparse$gentrificationChange - mean(testSparse$gentrificationChange))^2)
rsqRidge <- 1 - rssRidge/tssRidge
rsqRidge
plot(ridgeReg)
```

## lasso Regression
```{r}
runtime$lasso <- Sys.time()
x <- as.matrix(subset(trainSparse, select = -gentrificationChange))
y <- subset(trainSparse, select = gentrificationChange)
lassoReg <- cv.glmnet(x = x[],
                      y = y[,],
                      alpha = 1)
bestLambdaLasso <- lassoReg$lambda.min
bestFitLasso <- lassoReg$glmnet.fit
bestLassoReg <- glmnet(x[], y[,], alpha = 1, lambda = bestLambdaLasso)

testSparse$predictedGentrification <- predict(bestlassoReg,
                                              s = bestLambdalasso,
                                              newx = as.matrix(subset(testSparse, 
                                                                      select = -c(gentrificationChange,predictedGentrification)))[])
rsslasso <- sum((testSparse$predictedGentrification - testSparse$gentrificationChange)^2)
tsslasso <- sum((testSparse$gentrificationChange - mean(testSparse$gentrificationChange))^2)
rsqlasso <- 1 - rsslasso/tsslasso
rsqlasso
plot(lassoReg)
runtime$lasso <- Sys.time() - runtime$lasso
print(Sys.time())
```





```{r}
#When I run my conversion I'll have a mapping from one airbnb to a block. I'll have two observations, one in 2010 block and one in 2000 block. That #way I know which 2000 and 2010 tracts its in. 
#Condition on all the observations that have no change in the tract from 2000 to 2010.

#Order of Operations
#Code the maplinks from lat and longitude to blocks and tracts
#Pin down gentrification metric
#Pin down explanatory variables
  #Controls: population growth. Avg airbnb rating. Location rating on airbnb. 
  # Run a specification with fixed effects and not
  # A decision tree will rank which variables are most important

#TFIDF? Term frequency inverse document frequency. R should have built in function. It normalizes word frequency. 
#Monday Nov 15th send a rought draft to Gelman. 
```

Look into TFIDF

Will need to create a table with summary statistics of all variables except words
  Number of observations
  Mean
  Min 
  Max
  Median

Histogram of Gentrification
Map as an example showing the distribution

Table 6 from Jain but with theg eneral model.
It would be nice to do one for each of the cities, but maybe like 2 example cities. Imaigining a table 6 with the overall and with the Seattle and San Fracisisco
    

Look at Jain Paper for controls
  
Naive Bayes as a stretch goal??

  
What data frame do I need to run this model:
    Multiple objects. 
      Gentrification Variable
        Column with a row for each tract. Can be in the same data frame as the non word x variable.
      Non word X Variables
        Data frame with a row each tract. 
      
      Matrix which have a column for each word used in any review in any of the 28 MSAs
        Each row will be a census tract. The value of each cell will either be a quantifier for how many times the word shows up, or it could be a 1 or 0 for whether it shows up.
