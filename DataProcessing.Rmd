---
title: "DataProcessing"
author: "Leo Kitchell"
date: "11/27/2021"
output: html_document
---

# Purpose
The code below creates models to predict neighborhood (census tract) level gentrification between 2010-2014 and 2015-2019 using Airbnb data.

## Installing Packages & Libraries
```{r,warnings = FALSE}
library(stratification)
library(caTools)
library(tm)
library(SnowballC)
library(readr) 
library(randomForest)
library(tidyverse)
library(glmnet)

```


## Reading Data and Final Compilation
Data is aggregated at the airbnb listing level. Some final data collection steps are performed here because the data files were too large to push to github
```{r}
runtime <- list()
runtime$finalCompilation <- Sys.time()
### Stacking Review Data
path = "./InsideAirbnbData/Zipped_Data/"
file_list = list.files(path,'*.csv')
review_data <- array()
for (variable in file_list) {
  if(grepl("reviews",variable,fixed=TRUE)){
      review_data <- rbind(review_data,read.csv(paste(path,variable,sep = ""),encoding = "UTF-8"))
  } 
}

### Eliminating unnecessary data. Prefix "t" added representing they will be used for the text analysis portion. Alternately, "t" for tiny.
load("Compiled_Thesis_Data.RData")
tReviewData <- review_data %>% filter(!is.na(id)) #removes rows with NA listing IDs. 
tReviewData <- tReviewData %>% filter(date >= as.Date("2015-01-01") & date <= as.Date("2019-12-31"))  #only keep reviews between census collection time period: 2015-2019
tReviewData <- tReviewData %>% subset(select = c(listing_id,comments)) #keep only listing_id and text data
tReviewData <- aggregate(comments ~ listing_id, tReviewData, paste, collapse = " ") #combines all text data for a given listing into one row for that listing

tListingData <- listing_data %>% subset(select = c("id","bedrooms","bathrooms","price","minimum_nights",
                                                             "review_scores_rating","review_scores_location","license","tractFIPS2010"))
colnames(tListingData)[which(names(tListingData) == "price")] <- "listing_price" #renames column titled price to "listing_price". Prevents error of repeated column name in tSparse

airbnbData <- inner_join(tListingData,tReviewData,by = c("id"="listing_id"))

tCensusData <- gentrifiableCensusDataWithDistributionsWide %>% subset(select = 
                                                                        c("tractFIPS","NAME.2014","totalPop.2014","popGrowth","gentrificationChange"))

combinedData <- inner_join(tCensusData,airbnbData,by = c("tractFIPS"="tractFIPS2010"))
runtime$finalCompilation <- runtime$finalCompilation- Sys.time()
```


# To Do (Not in order):
 1) Try different values of sparsity. .95,.99,.995.   Look at the MSE for each model. Take the mean(MSE) for each model.
 2) Aggregate to the tract level. 
          Involves creating new variables: number of airbnbs per tract, average rating for listings in the tract, average price,
          average number of bedrooms with na.rm = TRUE, average star rating for the location. Probably use collapse to get all the words for the tract,
          and use summarize to get the summary statistics like average and n() for the other
 3) run model with all variables so i can see the relative importance
 4) DONE: add a list of city names as stopwords. Also state names. 
 5) Run model with TFDIF
 6) Investigate packages that will allow parallel processing (caret? foreach?)

## Creating Location Stopwords
```{r}
cities <- unlist(str_split(file_list,"listings"))
cities <- cities[substr(cities,0,1)=="_"]
cities <- substr(cities,2,nchar(cities)-4)
cities <- tolower(cities)
cities <- c(cities,"angeles","york","diego","francisco","minneapolis")

states <- read.csv("./OtherData/State_Fips_Codes.csv",colClasses = "character")
states <- inner_join(states,includedStates,by = c("State_FIPS"="state_FIPS"))
states <- tolower(unlist(states$State_Name))
locationWords <- c("north","east","south","west","northeast","northwest","southeast","southwest",states, cities)
```

## Cleaning Text Data
```{r}
runtime$corpus <- Sys.time()
#sampleSize <- 2000#nrow(combinedData)
#combinedDataSample <-combinedData[sample(nrow(combinedData), sampleSize), ]
#corpus <- Corpus(VectorSource(combinedDataSample$comments))

corpus <- Corpus(VectorSource(combinedData$comments))
corpus <- tm_map(corpus, PlainTextDocument)

corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, "New") #removing for place names (e.g. New York, New Jersey)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removeWords, c("airbnb",stopwords("english"),stopwords("spanish"),locationWords))  #remove stopwords
corpus <- tm_map(corpus, stemDocument)
runtime$corpus <- Sys.time() - runtime$corpus
```

## Adding Matrix of Word Frequencies to data
```{r}
frequencies <- DocumentTermMatrix(corpus)
sparse <- removeSparseTerms(frequencies, 0.98) #removes words which show up very few times across reviews.
#TODO: try different values of sparsity. .95,.99,.995.   Look at the MSE for each model. Take the mean(MSE) for each model.
tSparse <- as.data.frame(as.matrix(sparse))
colnames(tSparse) <- make.names(colnames(tSparse))
```

## Supplemental Data Cleaning
```{r}
predictorData <- subset(combinedDataSample,select = c(gentrificationChange))#,totalPop.2014,popGrowth))
tSparse <- cbind(predictorData,tSparse)
### Group tsparse by the tracts . Can take the average of the population because it is the same at the listing and the tract level (i.e. the listing data is just the tract data)
tSparse <- tSparse[,!duplicated(colnames(tSparse))] #removing columns with duplicate names

sum(!complete.cases(tSparse)) #number of rows with NAs. (There are 500 total)
combinedDataSample<-(combinedDataSample[!complete.cases(tSparse),]) #removing rows which contain 
incompleteCases<-(tSparse[!complete.cases(tSparse),])
tSparse<-(tSparse[complete.cases(tSparse),]) #removes all row without complete cases (500 when running on entire sample)

```

## Splitting Data into Testing and Training Sets
```{r}
index <- sample(seq_len(nrow(tSparse)), size = nrow(tSparse)*.7) #splits 70% of the data into a trainng set and 30% into a test set.
trainSparse <- tSparse[index, ]
testSparse <- tSparse[-index, ]
#save(list = c("trainSparse","testSparse","tSparse","combinedData"), file = "preModel.RData")
```

## Running Random Forest Model
```{r}
runtime$RF <- Sys.time()
mean(tSparse$gentrificationChange,na.rm = TRUE)  #baseline accuracy??
RF_model = randomForest(gentrificationChange ~ ., data=trainSparse,na.action = na.omit)
save(RF_model,file = "RF_model.RData")

predictRF = predict(RF_model, newdata=testSparse)
hist(predictRF)
varImpPlot(RF_model, n.var = 25)
RF_model$rsq

runtime$RF <- Sys.time() - runtime$RF
paste("finished at:",Sys.time())
```

$RF300
Time difference of 1.253781 mins
$RF1000
Time difference of 6.978322 mins
$RF150
Time difference of 1.977194 mins
Time difference of 1.605971 mins (ran again)

## Ridge Regression
```{r}
x <- as.matrix(subset(trainSparse, select = -gentrificationChange))
y <- subset(trainSparse, select = gentrificationChange)
ridgeReg <- cv.glmnet(x = x[],
                      y = y[,],
                      alpha = 0)
bestLambdaRidge <- ridgeReg$lambda.min
bestFitRidge <- ridgeReg$glmnet.fit
bestRidgeReg <- glmnet(x[], y[,], alpha = 0, lambda = bestLambdaRidge)

testSparse$predictedGentrification <- predict(bestRidgeReg,
                                              s = bestLambdaRidge,
                                              newx =as.matrix(subset(testSparse,
                                                                      select = -c(gentrificationChange)))[])
rssRidge <- sum((testSparse$predictedGentrification - testSparse$gentrificationChange)^2)
tssRidge <- sum((testSparse$gentrificationChange - mean(testSparse$gentrificationChange))^2)
rsqRidge <- 1 - rssRidge/tssRidge
rsqRidge
plot(ridgeReg)
```

## lasso Regression
```{r}
runtime$lasso <- Sys.time()
x <- as.matrix(subset(trainSparse, select = -gentrificationChange))
y <- subset(trainSparse, select = gentrificationChange)
lassoReg <- cv.glmnet(x = x[],
                      y = y[,],
                      alpha = 1)
bestLambdaLasso <- lassoReg$lambda.min
bestFitLasso <- lassoReg$glmnet.fit
bestLassoReg <- glmnet(x[], y[,], alpha = 1, lambda = bestLambdaLasso)

testSparse$predictedGentrification <- predict(bestlassoReg,
                                              s = bestLambdalasso,
                                              newx = as.matrix(subset(testSparse, 
                                                                      select = -c(gentrificationChange,predictedGentrification)))[])
rsslasso <- sum((testSparse$predictedGentrification - testSparse$gentrificationChange)^2)
tsslasso <- sum((testSparse$gentrificationChange - mean(testSparse$gentrificationChange))^2)
rsqlasso <- 1 - rsslasso/tsslasso
rsqlasso
plot(lassoReg)
runtime$lasso <- Sys.time() - runtime$lasso
```





```{r}
#When I run my conversion I'll have a mapping from one airbnb to a block. I'll have two observations, one in 2010 block and one in 2000 block. That #way I know which 2000 and 2010 tracts its in. 
#Condition on all the observations that have no change in the tract from 2000 to 2010.

#Order of Operations
#Code the maplinks from lat and longitude to blocks and tracts
#Pin down gentrification metric
#Pin down explanatory variables
  #Controls: population growth. Avg airbnb rating. Location rating on airbnb. 
  # Run a specification with fixed effects and not
  # A decision tree will rank which variables are most important

#TFIDF? Term frequency inverse document frequency. R should have built in function. It normalizes word frequency. 
#Monday Nov 15th send a rought draft to Gelman. 
```



Will need to create a table with summary statistics of all variables
  Number of observations
  Mean
  Min 
  Max
  Median

Histogram of Gentrification
Map as an example showing the distribution

Three groups of variables
  Traditional Airbnb
    Overall Stars
    Location Stars
    Number of Airbnbs
    Airbnb Price
  Non Traditional Airbnb
    Words
  Controls Variables at the Census Tract Level
    Population Change 2014-2019
    
    Look at Jain Paper for controls
    
  
What data frame do I need to run this model:
    Multiple objects. 
      Gentrification Variable
        Column with a row for each tract. Can be in the same data frame as the non word x variable.
      Non word X Variables
        Data frame with a row each tract. 
      
      Matrix which have a column for each word used in any review in any of the 28 MSAs
        Each row will be a census tract. The value of each cell will either be a quantifier for how many times the word shows up, or it could be a 1 or 0 for whether it shows up.
        
Mash all reviews together for each census tract. 
