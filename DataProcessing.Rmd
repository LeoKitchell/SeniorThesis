---
title: "DataProcessing"
author: "Leo Kitchell"
date: "12/03/2021"
output: html_document
---

# Purpose
The code below creates models to predict neighborhood (census tract) level gentrification between 2010-2014 and 2015-2019 using Airbnb data.

## Installing Packages & Libraries
```{r,warnings = FALSE}
library(caTools)
library(SnowballC)

library(randomForest)
library(glmnet)
library(tm)
library(tidyverse)
```


# Final Compilation of Listing and Review Data
Data is aggregated at the airbnb listing level. Some final data collection steps are performed here because the data files were too large to push to github

## Stacking Review Data
```{r}
runtime <- list()
runtime$finalCompilation <- Sys.time()
### Stacking Review Data
path = "./Data/InsideAirbnbData/Zipped_Data/"
file_list = list.files(path,'*.csv')
review_data <- array()
for (variable in file_list) {
  if(grepl("reviews",variable,fixed=TRUE)){
      review_data <- rbind(review_data,read.csv(paste(path,variable,sep = ""),encoding = "UTF-8"))
  } 
}
```

## Eliminating Review Data Outside of Time Window and Aggregating to Listing Level
```{r}
runtime$loadAndFilter <- Sys.time()
### Prefix "t" added representing they will be used for the text analysis portion. Alternately, "t" for tiny.
load("./Data/OtherData/Compiled_Thesis_Data.RData")
tReviewData <- review_data %>% filter(!is.na(id)) #removes rows with NA listing IDs. 
tReviewData$date <- as.Date(tReviewData$date)
tReviewData <- tReviewData %>% filter(date >= as.Date("2015-01-01") & date <= as.Date("2019-12-31"))  #only keep reviews between census collection time period: 2015-2019

tReviewData <- tReviewData %>% subset(select = c(listing_id,comments)) #keep only listing_id and text data
tReviewData <- tReviewData %>% group_by(listing_id) %>% mutate(numReviews = n()) #creates a feature for the number of reviews per listing
tReviewData <- aggregate(comments ~ listing_id + numReviews, tReviewData, paste, collapse = " ") #combines all text data for a given listing into one row for that listing
runtime$loadAndFilter <- Sys.time() - runtime$loadAndFilter
```

## Merging Airbnb (Listing Level) and Census Data (Tract Level)
```{r}
tListingData <- listing_data %>% subset(select = c("id","bedrooms","price","minimum_nights","review_scores_rating",
                                                             "review_scores_location","tractFIPS2010"))

colnames(tListingData)[which(names(tListingData) == "price")] <- "listing_price" #renames column titled price to "listing_price". 
                                                                                 #Prevents error of repeated column name in tSparse

airbnbData <- inner_join(tListingData,tReviewData,by = c("id"="listing_id"))

tCensusData <- gentrifiableCensusDataWithDistributionsWide %>% subset(select = c("tractFIPS","NAME.2014","totalPop.2014","popGrowth","gentrificationChange"))
tractData  <- inner_join(tCensusData,airbnbData,by = c("tractFIPS"="tractFIPS2010"))
```

## Aggregating Airbnb Data to Tract Level
```{r}
tractReviews <- tractData %>% subset(select = c(tractFIPS,comments))
tractReviews <- aggregate(comments ~ tractFIPS, tractReviews, paste,collapse = " ")

tractDataNoText <-  tractData %>% subset(select = -c(comments))

tractDataNoText <- tractDataNoText %>% group_by(tractFIPS) %>%
                                summarise(gentrificationChange = mean(gentrificationChange,na.rm = TRUE),
                                            totalPop.2014 = mean(totalPop.2014,na.rm = TRUE),
                                            popGrowth = mean(popGrowth,na.rm = TRUE),
                                            review_scores_rating = mean(review_scores_rating,na.rm = TRUE),
                                            review_scores_location = mean(review_scores_location,na.rm = TRUE),
                                            bedrooms = mean(bedrooms,na.rm = TRUE),
                                            listing_price = mean(parse_number(listing_price),na.rm = TRUE),
                                            numListings = n(),
                                            numReviews = sum(numReviews))

tractData <- inner_join(tractDataNoText,tractReviews, by = "tractFIPS")
save(tractDataNoText, file = "./Data/OtherData/tractDataNoText.RData")
rm(list = setdiff(ls(), c("tractDataNoText","tractData","listing_data"))) #cleaning up environment for storage reasons
runtime$finalCompilation <- Sys.time() - runtime$finalCompilation
```

## Creating Location Stopwords
```{r}
includedStates <- listing_data %>% group_by(listing_data$state) %>% summarize() %>% drop_na()
names(includedStates)<-"state_FIPS"
file_list = list.files("./InsideAirbnbData/Zipped_Data/",'*.csv')
cities <- unlist(str_split(file_list,"listings"))
cities <- cities[substr(cities,0,1)=="_"]
cities <- substr(cities,2,nchar(cities)-4)
cities <- tolower(cities)
cities <- c(cities,"angeles","york","diego","francisco","minneapolis")

states <- read.csv("./Data/OtherData/State_Fips_Codes.csv",colClasses = "character")
states <- inner_join(states,includedStates,by = c("State_FIPS"="state_FIPS"))
states <- tolower(unlist(states$State_Name))
locationWords <- c("north","east","south","west","northeast","northwest","southeast","southwest",states, cities)
```

## Cleaning Text Data
Removes formatting, stopwords, and stems text. Takes approximately 2 hours to run.
```{r}
runtime$corpus <- Sys.time()
corpus <- Corpus(VectorSource(tractData$comments)) 
corpus <- tm_map(corpus, PlainTextDocument)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, "New") #removing for place names (e.g. New York, New Jersey)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removeWords, c("airbnb",stopwords("english"),stopwords("spanish"),locationWords))  #remove stopwords
corpus <- tm_map(corpus, stemDocument)
#save(corpus,file = "./Data/OtherData/processedFullCorpus.RData")  #commented out so as to not accidentally overwrite saved data
runtime$corpus <- Sys.time() - runtime$corpus
```

# Model Data Creation
Creating training data for models from the processed corpus. Provides a checkpoint to start from after crashes, etc. 
## Loading Relevant Data and Libraries as Checkpoint
```{r}
library(tm)
library(tidyverse)
lapply(c("./Data/OtherData/processedFullCorpus.RData","./Data/OtherData/tractDataNoText.RData"),load,envir=.GlobalEnv) #loads necessary data
```

## Creating Matrices of Word Frequencies
Both Bag of Words and TF IDF are created. Different sparsity thresholds are also tested
```{r}
sparsities <- c(.4,.5,.6,.7,.8,.9,.95,.99)
frequencies <- DocumentTermMatrix(corpus)
tfidFrequencies <- DocumentTermMatrix(corpus,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))

sparseList       <- list()
tSparseList      <- list()
sparseListTFIDF   <- list()
tSparseListTFIDF  <- list()

ii<-1
while (ii < length(sparsities)+1) {
  sparseList[[ii]]         <- removeSparseTerms(frequencies, sparsities[ii])
  sparseListTFIDF[[ii]]     <- removeSparseTerms(tfidFrequencies, sparsities[ii])
  tSparseList[[ii]]          <- as.data.frame(as.matrix(sparseList[[ii]]))
  tSparseListTFIDF[[ii]]      <- as.data.frame(as.matrix(sparseListTFIDF[[ii]]))
  colnames(tSparseList[[ii]])     <- make.names(colnames(tSparseList[[ii]]))
  colnames(tSparseListTFIDF[[ii]]) <- make.names(colnames(tSparseListTFIDF[[ii]]))
  ii <- ii+1
}
```

## Creating and Combining Predictor Data
Training and Testing data for different sparsity thresholds are created. 
Similar sets are created for a 1% data sample for faster parameter tuning and troubleshoot.
```{r}
trainProportion <- 0.7     #Uses 70% of cases as a training set and 30% as a test case
speedFactor <- .1         #Value between 0 and 1. Percent of tracts to use in training and testing set. When running full model set to 1. 

#Bag of Words Section
indexSparseTrain <- list()
trainSparseList  <- list()
testSparseList   <- list()
indexSpeedSparse <- list()
speedSparseList  <- list()
indexSpeedSparseTrain <- list()
speedTrainSparseList  <- list()
speedTestSparseList   <- list()

#TF IDF Section
indexSparseTrainTFIDF <- list()
trainSparseListTFIDF  <- list()
testSparseListTFIDF   <- list()
indexSpeedSparseTFIDF <- list()
speedSparseListTFIDF  <- list()
indexSpeedSparseTrainTFIDF <- list()
speedTrainSparseListTFIDF  <- list()
speedTestSparseListTFIDF   <- list()

ii<-1
while (ii < length(tSparseList)+1) {
  ##Bag of Words Section
  tSparseList[[ii]]     <- bind_cols(tractDataNoText,tSparseList[[ii]])                 #combine text and structured features
  tSparseList[[ii]]     <- tSparseList[[ii]][,!duplicated(colnames(tSparseList[[ii]]))]           #remove columns with duplicate names
  tSparseList[[ii]]     <- tSparseList[[ii]][complete.cases(tSparseList[[ii]]),]                  #remove rows with NAs and NaNs
  tSparseList[[ii]]     <- subset(tSparseList[[ii]],select = -c(tractFIPS))                       #remove tractFIPS from dataframe  
  seed <- set.seed(2021)
  indexSparseTrain[[ii]]  <- sample(seq_len(nrow(tSparseList[[ii]])), size = nrow(tSparseList[[ii]])*trainProportion) #create a training and testing dataset
  trainSparseList[[ii]] <- tSparseList[[ii]][indexSparseTrain[[ii]],]    
  testSparseList[[ii]]  <- tSparseList[[ii]][-indexSparseTrain[[ii]],]
  
  indexSpeedSparse[[ii]]  <- sample(seq_len(nrow(tSparseList[[ii]])), size = nrow(tSparseList[[ii]])*speedFactor)   #create a subset of the full dataset to train small models on
  speedSparseList[[ii]] <- tSparseList[[ii]][indexSpeedSparse[[ii]],]
  
  indexSpeedSparseTrain[[ii]]  <- sample(seq_len(nrow(speedSparseList[[ii]])), size = nrow(speedSparseList[[ii]])*trainProportion)
  speedTrainSparseList[[ii]] <-speedSparseList[[ii]][indexSpeedSparseTrain[[ii]],] 
  speedTestSparseList[[ii]] <- speedSparseList[[ii]][-indexSpeedSparseTrain[[ii]],] 
  
  ##TF IDF Section
  tSparseListTFIDF[[ii]]     <- bind_cols(tractDataNoText,tSparseListTFIDF[[ii]])                 #combine text and structured features
  tSparseListTFIDF[[ii]]     <- tSparseListTFIDF[[ii]][,!duplicated(colnames(tSparseListTFIDF[[ii]]))]           #remove columns with duplicate names
  tSparseListTFIDF[[ii]]     <- tSparseListTFIDF[[ii]][complete.cases(tSparseListTFIDF[[ii]]),]                  #remove rows with NAs and NaNs
  tSparseListTFIDF[[ii]]     <- subset(tSparseListTFIDF[[ii]],select = -c(tractFIPS))                       #remove tractFIPS from dataframe  
  
  indexSparseTrainTFIDF[[ii]]  <- sample(seq_len(nrow(tSparseListTFIDF[[ii]])), size = nrow(tSparseListTFIDF[[ii]])*trainProportion) #create a training and testing dataset
  trainSparseListTFIDF[[ii]] <- tSparseListTFIDF[[ii]][indexSparseTrainTFIDF[[ii]],]    
  testSparseListTFIDF[[ii]]  <- tSparseListTFIDF[[ii]][-indexSparseTrainTFIDF[[ii]],]
  
  indexSpeedSparseTFIDF[[ii]]  <- sample(seq_len(nrow(tSparseListTFIDF[[ii]])), size = nrow(tSparseListTFIDF[[ii]])*speedFactor)   #create a subset of the full dataset to train small models on
  speedSparseListTFIDF[[ii]] <- tSparseListTFIDF[[ii]][indexSpeedSparseTFIDF[[ii]],]
  
  indexSpeedSparseTrainTFIDF[[ii]]  <- sample(seq_len(nrow(speedSparseListTFIDF[[ii]])), size = nrow(speedSparseListTFIDF[[ii]])*trainProportion)
  speedTrainSparseListTFIDF[[ii]] <-speedSparseListTFIDF[[ii]][indexSpeedSparseTrainTFIDF[[ii]],] 
  speedTestSparseListTFIDF[[ii]] <- speedSparseListTFIDF[[ii]][-indexSpeedSparseTrainTFIDF[[ii]],]  
  ii<-ii+1 
}

testData <- list(testSparseList[[1]]$gentrificationChange,testSparseListTFIDF[[1]]$gentrificationChange,
              speedTestSparseList[[1]]$gentrificationChange,speedTestSparseListTFIDF[[1]]$gentrificationChange)
save(testData,file = "./Data/OtherData/testData.RData")
```
# Random Forest Models - Training and Testing
Trains a variety of random forest models using different parameters and data sets. Small data models can be unstable in multithreaded operations. 
## Large RFs
### Random Forests - Full Data, Large Model, Bag of Words
Uses Full data set and 200 trees
```{r, warning=FALSE}
library(caret)
library(ranger)
runtime$largeRangers <- Sys.time()
rangerLargeModels <- list()
predictLargeRangers <- list()
impPlotsLargeRanger <- list(list())

ii<-1
while (ii <= length(tSparseList)) {
      rangerLargeModels[[ii]] <- caret::train(
                                      x = tSparseList[[ii]][,-1],
                                      y = tSparseList[[ii]][,1][[1]],
                                      method = "ranger",
                                      importance = "impurity",
                                      num.trees = 200)
#      predictLargeRangers[[ii]] <- predict(rangerLargeModels[[ii]],newdata = testSparseList[[ii]])
      impPlotsLargeRanger[[ii]] <- plot(varImp(rangerLargeModels[[ii]]),top = 15)
      save(list = c("rangerLargeModels","impPlotsLargeRanger","predictLargeRangers"),file = "./Data/OtherData/largeRangerResults.RData")
      print(paste("Model",ii,"finished and saved at",Sys.time()))
      ii<-ii+1
}

# testMSELargeRanger <- double()
# ii<-1
# while (ii <= length(rangerLargeModels)) {
#   testMSELargeRanger[ii] <- sum((predictLargeRangers[[ii]]-testData[[1]])**2)/length(testData[[1]])
#   ii <- ii+1
# }

runtime$largeRangers <- Sys.time() - runtime$largeRangers

```

### Random Forests - Full Data, Large Model, TF IDF
Uses the Full data set and 200 trees
```{r,warning=FALSE}
library(caret)
library(ranger)
runtime$largeRangersTFIDF <- Sys.time()
rangerLargeModelsTFIDF <- list()
predictLargeRangersTFIDF <- list()
impPlotsLargeRangerTFIDF <- list()

ii<-1
while (ii <= length(tSparseListTFIDF)) {
      rangerLargeModelsTFIDF[[ii]] <- caret::train(
                                      x = tSparseListTFIDF[[ii]][,-1],
                                      y = tSparseListTFIDF[[ii]][,1][[1]],
                                      method = "ranger",
                                      importance = "impurity",
                                      num.trees = 200)
#      predictLargeRangersTFIDF[[ii]] <- predict(rangerLargeModelsTFIDF[[ii]],newdata = testSparseListTFIDF[[ii]])
      impPlotsLargeRangerTFIDF[[ii]] <- plot(varImp(rangerLargeModelsTFIDF[[ii]]),top = 15)
      save(list = c("rangerLargeModelsTFIDF","impPlotsLargeRangerTFIDF","predictLargeRangersTFIDF"),file = "./Data/OtherData/largeRangerResultsTFIDF.RData")
      print(paste("Model",ii,"finished and saved at",Sys.time()))
      ii<-ii+1
}

# 
# testMSELargeRangerTFIDF <- double()
# ii<-1
# while (ii <= length(rangerLargeModelsTFIDF)) {
#   testMSELargeRangerTFIDF[ii] <- sum((predictLargeRangersTFIDF[[ii]]-testData[[2]])**2)/length(testData[[2]])
#   ii <- ii+1
# }
runtime$largeRangersTFIDF <- Sys.time() - runtime$largeRangersTFIDF

impPlotsLargeRangerTFIDF

```

### Random Forest - Only Structured Features,  Full Data, Large Model
```{r,warning=FALSE}
runtime$largeRangersStructured <- Sys.time()
rangerLargeModelsStructured <- list()
predictLargeRangersStructured <- list()
impPlotsLargeRangerStructured <- list()
tSparseListStructured <- lapply(trainSparseList, "[", c(1:9))
#trainSparseListStructured <- lapply(trainSparseList, "[", c(1:9))
#testSparseListStructured  <- lapply(testSparseList,"[", c(1:9))


ii<-1
while (ii <= length(trainSparseList)) {
      rangerLargeModelsStructured[[ii]] <- caret::train(
                                      x = tSparseListStructured[[ii]][,-1],
                                      y = tSparseListStructured[[ii]][,1][[1]],
                                      method = "ranger",
                                      importance = "impurity",
                                      num.trees = 200)
#      predictLargeRangersStructured[[ii]] <- predict(rangerLargeModelsStructured[[ii]],newdata = testSparseList[[ii]])
      impPlotsLargeRangerStructured[[ii]] <- plot(varImp(rangerLargeModelsStructured[[ii]]),top = 15)
      save(list = c("rangerLargeModelsStructured","impPlotsLargeRangerStructured","predictLargeRangersStructured"),
           file = "./Data/OtherData/largeRangerResultsStructured.RData")
      print(paste("Model",ii,"finished and saved at",Sys.time()))
      ii<-ii+1
}


# testMSELargeRangerStructured <- double()
# ii<-1
# while (ii <= length(rangerLargeModelsStructured)) {
#   testMSELargeRangerStructured[ii] <- sum((predictLargeRangersStructured[[ii]]-testData[[1]])**2)/length(testData[[1]])
#   ii <- ii+1
# }
# testMSELargeRangerStructured
ii<-1
while (ii <= length(trainSparseList)) {
  print(paste("Model with sparsity =",ii," has r squared:",rangerLargeModelsStructured[[ii]]$finalModel$r.squared))
  print(paste("Model with sparsity =",ii," has MSE:",rangerLargeModelsStructured[[ii]]$finalModel$prediction.error))
  ii <- ii+1
}
runtime$largeRangersStructured <- Sys.time() - runtime$largeRangersStructured
```

### Random Forest - Only Structured Features, Full Data, Large Model, TFIDF
```{r}
runtime$largeRangersStructuredTFIDF <- Sys.time()
rangerLargeModelsStructuredTFIDF <- list()
predictLargeRangersStructuredTFIDF <- list()
impPlotsLargeRangerStructuredTFIDF <- list()
tSparseListStructuredTFIDF <- lapply(trainSparseListTFIDF, "[", c(1:9))
trainSparseListStructuredTFIDF <- lapply(trainSparseListTFIDF, "[", c(1:9))
testSparseListStructuredTFIDF  <- lapply(testSparseListTFIDF,"[", c(1:9))


ii<-1
while (ii <= length(trainSparseListTFIDF)) {
      rangerLargeModelsStructuredTFIDF[[ii]] <- caret::train(
                                      x = tSparseListStructuredTFIDF[[ii]][,-1],
                                      y = tSparseListStructuredTFIDF[[ii]][,1][[1]],
                                      method = "ranger",
                                      importance = "impurity",
                                      num.trees = 200)
 #     predictLargeRangersStructuredTFIDF[[ii]] <- predict(rangerLargeModelsStructuredTFIDF[[ii]],newdata = testSparseListTFIDF[[ii]])
      impPlotsLargeRangerStructuredTFIDF[[ii]] <- plot(varImp(rangerLargeModelsStructuredTFIDF[[ii]]),top = 15)
      save(list = c("rangerLargeModelsStructuredTFIDF","impPlotsLargeRangerStructuredTFIDF","predictLargeRangersStructuredTFIDF"),
           file = "./Data/OtherData/largeRangerResultsStructuredTFIDF.RData")
      print(paste("Model",ii,"finished and saved at",Sys.time()))
      ii<-ii+1
}

# testMSELargeRangerStructuredTFIDF <- double()
# ii<-1
# while (ii <= length(rangerLargeModelsStructuredTFIDF)) {
#   testMSELargeRangerStructuredTFIDF[ii] <- sum((predictLargeRangersStructuredTFIDF[[ii]]-testData[[2]])**2)/length(testData[[2]])
#   ii <- ii+1
# }
# testMSELargeRangerStructuredTFIDF
runtime$largeRangersStructuredTFIDF <- Sys.time() - runtime$largeRangersStructuredTFIDF
```

### Random Forests - Full Data, Extra Large Model, Bag of Words
Uses Full data set and 500 trees
```{r, warning=FALSE}
runtime$extraLargeRangers <- Sys.time()
rangerExtraLargeModels <- list()
predictExtraLargeRangers <- list()
impPlotsExtraLargeRanger <- list(list())

ii<-1
while (ii <= length(trainSparseList)) {
      rangerExtraLargeModels[[ii]] <- caret::train(
                                      x = tSparseList[[ii]][[ii]][,-1],
                                      y = tSparseList[[ii]][[ii]][,1][[1]],
                                      method = "ranger",
                                      importance = "impurity",
                                      num.trees = 500)
#      predictExtraLargeRangers[[ii]] <- predict(rangerExtraLargeModels[[ii]],newdata = testSparseList[[ii]])
      impPlotsExtraLargeRanger[[ii]] <- plot(varImp(rangerExtraLargeModels[[ii]]),top = 15)
      save(list = c("rangerExtraLargeModels","impPlotsExtraLargeRanger","predictExtraLargeRangers"),file = "./Data/OtherData/extraLargeRangerResults.RData")
      print(paste("Model",ii,"finished and saved at",Sys.time()))
      ii<-ii+1
}

# testMSEExtraLargeRanger <- double()
# ii<-1
# while (ii <= length(rangerExtraLargeModels)) {
#   testMSEExtraLargeRanger[ii] <- sum((predictLargeRangers[[ii]]-testData[[1]])**2)/length(testData[[1]])
#   ii <- ii+1
# }

runtime$extraLargeRangers <- Sys.time() - runtime$extraLargeRangers
```

### Random Forests - Full Data, Extra Large Model, TF IDF
Uses the Full data set and 500 trees
```{r}
runtime$extraLargeRangersTFIDF <- Sys.time()
rangerExtraLargeModelsTFIDF <- list()
predictExtraLargeRangersTFIDF <- list()
impPlotsExtraLargeRangerTFIDF <- list()

ii<-1
while (ii <= length(trainSparseListTFIDF)) {
      rangerExtraLargeModelsTFIDF[[ii]] <- caret::train(gentrificationChange ~ .,
                                      data = tSparseListTFIDF[[ii]],
                                      method = "ranger",
                                      importance = "impurity",
                                      num.trees = 500)
#      predictExtraLargeRangersTFIDF[[ii]] <- predict(rangerExtraLargeModelsTFIDF[[ii]],newdata = testSparseListTFIDF[[ii]])
      impPlotsExtraLargeRangerTFIDF[[ii]] <- plot(varImp(rangerExtraLargeModelsTFIDF[[ii]]),top = 15)
      save(list = c("rangerExtraLargeModelsTFIDF","impPlotsExtraLargeRangerTFIDF","predictExtraLargeRangersTFIDF","runtime"),
           file = "./Data/OtherData/extraLargeRangerResultsTFIDF.RData")
      print(paste("Model",ii,"finished and saved at",Sys.time()))
      ii<-ii+1
}
# ii<-1
# while (ii <= length(rangerExtraLargeModels)) {
#   testMSEExtraLargeRanger[ii] <- sum((predictLargeRangers[[ii]]-testData[[2]])**2)/length(testData[[2]])
#   ii <- ii+1
# }
runtime$extraLargeRangersTFIDF <- Sys.time() - runtime$extraLargeRangersTFIDF
```


## Random Forests - Small Data, Small Model, Bag of Words, Only Structured
Uses 1% data set and 15 trees
```{r, warning=FALSE}
runtime$smallRangersStructured <- Sys.time()
rangerSmallModelsStructured <- list()
predictSmallRangersStructured <- list()
impPlotsSmallRangerStructured <- list()
speedSparseListStructured <- lapply(speedSparseList, "[", c(1:9))
#speedTestSparseListStructured  <- lapply(speedTestSparseList,"[", c(1:9))


ii<-1
while (ii <= length(speedTrainSparseList)) {
      rangerSmallModelsStructured[[ii]] <- caret::train(
                                      x = speedSparseListStructured[[ii]][,-1],
                                      y = speedSparseListStructured[[ii]][,1][[1]],
                                      method = "ranger",
                                      importance = "impurity",
                                      num.trees = 15)
#      predictSmallRangersStructured[[ii]] <- predict(rangerSmallModels[[ii]],newdata = speedTestSparseListStructured[[ii]])
      impPlotsSmallRangerStructured[[ii]] <- plot(varImp(rangerSmallModelsStructured[[ii]]),top = 15)
      save(list = c("rangerSmallModelsStructured","impPlotsSmallRangerStructured","predictSmallRangersStructured"),
           file = "./Data/OtherData/smallRangerResultsStructured.RData")
      print(paste("Model",ii,"finished and saved at",Sys.time()))
      ii<-ii+1
}

ii<-1
while (ii <= length(rangerSmallModelsStructured)) {
   print(rangerSmallModelsStructured[[ii]]$finalModel$prediction.error)
   ii <- ii+1
}
runtime$smallRangersStructured <- Sys.time() - runtime$smallRangersStructured
```

## Random Forests - Small Data, Small Model, TF IDF
Uses 1% data set and 15 trees
```{r}
runtime$smallRangersTFIDF <- Sys.time()
rangerSmallModelsTFIDF <- list()
predictSmallRangersTFIDF <- list()
impPlotsSmallRangerTFIDF <- list()

ii<-1
while (ii <= length(speedTrainSparseListTFIDF)) {
      rangerSmallModelsTFIDF[[ii]] <- caret::train(gentrificationChange ~ .,
                                      data = speedSparseListTFIDF[[ii]],
                                      method = "ranger",
                                      importance = "impurity",
                                      num.trees = 15)
      predictSmallRangersTFIDF[[ii]] <- predict(rangerSmallModelsTFIDF[[ii]],newdata = speedTestSparseListTFIDF[[ii]])
      impPlotsSmallRangerTFIDF[[ii]] <- plot(varImp(rangerSmallModelsTFIDF[[ii]]),top = 15)
      print(paste("Model",ii,"finished at",Sys.time()))
      ii<-ii+1
}


save(list = c("rangerSmallModelsTFIDF","impPlotsSmallRangerTFIDF","predictSmallRangersTFIDF"),file = "./Data/OtherData/smallRangerResultsTFIDF.RData")

ii<-1
while (ii < 7) {
  print(paste("Model TFIDF with sparsity =",ii," has r squared:",rangerSmallModelsTFIDF[[ii]]$finalModel$r.squared))
  print(paste("Model TFIDF with sparsity =",ii," has MSE:",rangerSmallModelsTFIDF[[ii]]$finalModel$prediction.error))
  ii <- ii+1
}
runtime$smallRangersTFIDF <- Sys.time() - runtime$smallRangersTFIDF
impPlotsLargeRangerStructured
ii<-1
while (ii < 9) {
  #print(paste("Model TFIDF with sparsity =",ii," has r squared:",rangerLargeModelsStructured[[ii]]$finalModel$r.squared))
  print(paste("Model TFIDF with sparsity =",ii," has MSE:",rangerLargeModelsTFIDF[[ii]]$finalModel$prediction.error))
  ii <- ii+1
}
```


# Other Model Training and Testing
Regularized linear regression models are evaluated. Performance was consistently inferior to Random Forest models, so code may not be maintained. 
## Ridge Regression
```{r}
x <- as.matrix(subset(trainSparse, select = -gentrificationChange))
y <- subset(trainSparse, select = gentrificationChange)
ridgeReg <- cv.glmnet(x = x[],
                      y = y[,],
                      alpha = 0)
bestLambdaRidge <- ridgeReg$lambda.min
bestFitRidge <- ridgeReg$glmnet.fit
bestRidgeReg <- glmnet(x[], y[,], alpha = 0, lambda = bestLambdaRidge)

testSparse$predictedGentrification <- predict(bestRidgeReg,
                                              s = bestLambdaRidge,
                                              newx =as.matrix(subset(testSparse,
                                                                      select = -c(gentrificationChange)))[])
rssRidge <- sum((testSparse$predictedGentrification - testSparse$gentrificationChange)^2)
tssRidge <- sum((testSparse$gentrificationChange - mean(testSparse$gentrificationChange))^2)
rsqRidge <- 1 - rssRidge/tssRidge
rsqRidge
plot(ridgeReg)
```

## lasso Regression
```{r}
runtime$lasso <- Sys.time()
x <- as.matrix(subset(trainSparse, select = -gentrificationChange))
y <- subset(trainSparse, select = gentrificationChange)
lassoReg <- cv.glmnet(x = x[],
                      y = y[,],
                      alpha = 1)
bestLambdaLasso <- lassoReg$lambda.min
bestFitLasso <- lassoReg$glmnet.fit
bestLassoReg <- glmnet(x[], y[,], alpha = 1, lambda = bestLambdaLasso)

testSparse$predictedGentrification <- predict(bestlassoReg,
                                              s = bestLambdalasso,
                                              newx = as.matrix(subset(testSparse, 
                                                                      select = -c(gentrificationChange,predictedGentrification)))[])
rsslasso <- sum((testSparse$predictedGentrification - testSparse$gentrificationChange)^2)
tsslasso <- sum((testSparse$gentrificationChange - mean(testSparse$gentrificationChange))^2)
rsqlasso <- 1 - rsslasso/tsslasso
rsqlasso
plot(lassoReg)
runtime$lasso <- Sys.time() - runtime$lasso
print(Sys.time())
```




# Random Forest Results Visualisation

## Loading Trained Model Data and Libraries as Checkpoint
```{r}
library(ranger)
library(tidyverse)
results <- c(list.files("./Data/OtherData/","*RangerResults.RData"),list.files("./Data/OtherData/","*RangerResultsTFIDF.RData"))
lapply(X = paste("./Data/OtherData/",results,sep=""),FUN = load,envir=.GlobalEnv,) #loads models
```

## Creating Data Frame of Large Model Results
```{r}
numRangers <- length(ls()[substr(ls(),1,6)=="ranger"])
ls()[substr(ls(),1,6)=="ranger"]
sevenLevelSparsity <- c(.5,.6,.7,.8,.9,.95,.99)
eightLevelSparsity <- c(.4,.5,.6,.7,.8,.9,.95,.99)

ii<-1
rangerResults <- data.frame(matrix(ncol = 8,nrow = 1))
colnames(rangerResults) <- c("modelType","trainDataSize","sparsity","rsquared","MSE","NumTrees","mTry","minNodeSize")
rangerResults[1,] <- 1
while (ii <= length(rangerExtraLargeModels)) {
  row1  <- c("Bag of Words",rangerLargeModels[[ii]]$finalModel$num.samples,eightLevelSparsity[ii],
           rangerLargeModels[[ii]]$finalModel$r.squared,
           rangerLargeModels[[ii]]$finalModel$prediction.error,
           rangerLargeModels[[ii]]$finalModel$num.trees,
           rangerLargeModels[[ii]]$finalModel$mtry,
           rangerLargeModels[[ii]]$finalModel$min.node.size)
  row2 <- c("TF IDF",rangerLargeModels[[ii]]$finalModel$num.samples,eightLevelSparsity[ii],
           rangerLargeModelsTFIDF[[ii]]$finalModel$r.squared,
           rangerLargeModelsTFIDF[[ii]]$finalModel$prediction.error,
           rangerLargeModelsTFIDF[[ii]]$finalModel$num.trees,
           rangerLargeModelsTFIDF[[ii]]$finalModel$mtry,
           rangerLargeModelsTFIDF[[ii]]$finalModel$min.node.size)
  row3 <- c("Bag of Words",rangerExtraLargeModelsTFIDF[[ii]]$finalModel$num.samples,eightLevelSparsity[ii],
           rangerExtraLargeModels[[ii]]$finalModel$r.squared,
           rangerExtraLargeModels[[ii]]$finalModel$prediction.error,
           rangerExtraLargeModels[[ii]]$finalModel$num.trees,
           rangerExtraLargeModels[[ii]]$finalModel$mtry,
           rangerExtraLargeModels[[ii]]$finalModel$min.node.size)  
  row4 <- c("TF IDF",rangerExtraLargeModelsTFIDF[[ii]]$finalModel$num.samples,eightLevelSparsity[ii],
           rangerExtraLargeModelsTFIDF[[ii]]$finalModel$r.squared,
           rangerExtraLargeModelsTFIDF[[ii]]$finalModel$prediction.error,
           rangerExtraLargeModelsTFIDF[[ii]]$finalModel$num.trees,
           rangerExtraLargeModelsTFIDF[[ii]]$finalModel$mtry,
           rangerExtraLargeModelsTFIDF[[ii]]$finalModel$min.node.size)
  rangerResults <- rbind(rangerResults,row1,row2,row3,row4)
  ii <- ii+1
}
rangerResults <- rangerResults[2:nrow(rangerResults),]
rangerResults$modelName <- paste(rangerResults$modelType,rangerResults$NumTrees,sep="; nTrees: ")
```

## Plotting Model Results
```{r}
library(ggplot2)

ggplot(data = rangerResults, aes(x = sparsity, y = as.double(MSE), group = modelName)) +
      geom_line(aes(color = modelName)) + labs(title = "Model Accuracy by Sparsity and NTrees")
  ylab("Mean Squared Error")
```
# Final Models
```{r}
runtime <- list()
mtryOptimal <- 913
sparseOptimal <- 6
ntreeOptimal <- 500

tGrid <- expand.grid(
                     .mtry = mtryOptimal,
                     .splitrule = "variance",
                     .min.node.size = 5)
runtime$optimalRanger1 <- Sys.time()
# optimalRanger <- caret::train(x = tSparseList[[sparseOptimal]][,-1],
#                               y = tSparseList[[sparseOptimal]][,1][[1]],
#                               method = "ranger",
#                               importance = "impurity",
#                               num.trees = ntreeOptimal,
#                               tuneGrid = tGrid)
# impPlotsOptimalRanger <- plot(varImp(optimalRanger),top = 15)
# save(list = c("optimalRanger","impPlotsOptimalRanger"),file = "./Data/OtherData/optimalRangerResults.RData")
# print(paste("Model finished and saved at",Sys.time()))
runtime$optimalRanger1 <- Sys.time() - runtime$optimalRanger1


runtime$optimalRanger2 <- Sys.time()
sparseOptimal <- 7
optimalRanger2 <- caret::train(x = tSparseList[[sparseOptimal]][,-1],
                              y = tSparseList[[sparseOptimal]][,1][[1]],
                              method = "ranger",
                              importance = "impurity",
                              num.trees = ntreeOptimal,
                              tuneGrid = tGrid)
impPlotsOptimalRanger2 <- plot(varImp(optimalRanger2),top = 15)
save(list = c("optimalRanger2","impPlotsOptimalRanger2"),file = "./Data/OtherData/optimalRangerResults2.RData")
print(paste("Model finished and saved at",Sys.time()))
runtime$optimalRanger2 <- Sys.time() - runtime$optimalRanger2


runtime$optimalRanger3 <- Sys.time()
sparseOptimal <- 8
optimalRanger3 <- caret::train(x = tSparseList[[sparseOptimal]][,-1],
                              y = tSparseList[[sparseOptimal]][,1][[1]],
                              method = "ranger",
                              importance = "impurity",
                              num.trees = ntreeOptimal,
                              tuneGrid = tGrid)
impPlotsOptimalRanger3 <- plot(varImp(optimalRanger3),top = 15)
save(list = c("optimalRanger3","impPlotsOptimalRanger3"),file = "./Data/OtherData/optimalRangerResults3.RData")
print(paste("Model finished and saved at",Sys.time()))
runtime$optimalRanger3 <- Sys.time() - runtime$optimalRanger3

```


# Prediction
## Predicting Gentrification 2019-2021
Uses the trained model to predict gentrifiaction using the reviews between 2019 and 2021
```{r}


```

## Mapping Predicted Gentrification
```{r}

```




# Notes

```{r}
#When I run my conversion I'll have a mapping from one airbnb to a block. I'll have two observations, one in 2010 block and one in 2000 block. That #way I know which 2000 and 2010 tracts its in. 
#Condition on all the observations that have no change in the tract from 2000 to 2010.

#Order of Operations
#Code the maplinks from lat and longitude to blocks and tracts
#Pin down gentrification metric
#Pin down explanatory variables
  #Controls: population growth. Avg airbnb rating. Location rating on airbnb. 
  # Run a specification with fixed effects and not
  # A decision tree will rank which variables are most important

#TFIDF? Term frequency inverse document frequency. R should have built in function. It normalizes word frequency. 
#Monday Nov 15th send a rought draft to Gelman. 
```

Look into TFIDF

Will need to create a table with summary statistics of all variables except words
  Number of observations
  Mean
  Min 
  Max
  Median

Histogram of Gentrification
Map as an example showing the distribution

Table 6 from Jain but with theg eneral model.
It would be nice to do one for each of the cities, but maybe like 2 example cities. Imaigining a table 6 with the overall and with the Seattle and San Fracisisco
    

Look at Jain Paper for controls
  
Naive Bayes as a stretch goal??

  
What data frame do I need to run this model:
    Multiple objects. 
      Gentrification Variable
        Column with a row for each tract. Can be in the same data frame as the non word x variable.
      Non word X Variables
        Data frame with a row each tract. 
      
      Matrix which have a column for each word used in any review in any of the 28 MSAs
        Each row will be a census tract. The value of each cell will either be a quantifier for how many times the word shows up, or it could be a 1 or 0 for whether it shows up.



# Thesis Writing & Stucture Details
## Thesis Formatting
Footnotes not endnotes.
Word, probably not latex
references are (Jain 2021).
In line images, not at the end.
Table of summary stats? Package in R that saves table to a .doc file and then I can import that in to a word document.

## Thesis Layout
Intro
  Talks about motivation
  Preview of results
Literature Review
  General quantitve gentrification papers
  Jain paper
    The most related paper to mine is this Jain paper. Highlight what I've contributed beyond this paper.
        Generalizing to a larger sample of cities
        Letting the model decide important words rather than pre-processing
Data
  Describe census data and the airbnb data
  Describe gentrification index
  Gentrification maps
  Summary statistics of variables
Specification
  Used random forest
    Footnotes can say that I used ranger
  Optimal Parameter Values
  Say that I used Bag of Words 
  We also tried TF IDF and regularized linear regressions
  Did optimization and used MSE as the criteria to select the best model. From here on out only talk about the best Model.
Results
  Importance Factors Ranking of Features
  
Predictions
  Using data from Jan 2020-Present we predict gentrification to be highest in these areas. Potentially map of areas. 
  
  Can include discussion
Conclusion
  Model can be extended 

```{r}

ii<-8
a <- list()
tuneRanger::

while (ii <= length(speedTrainSparseListTFIDF)){
  a[ii] <- tuneMtryFast(gentrificationChange ~ .,
                                      data = speedSparseListTFIDF[[ii]],
                                      method = "ranger",
                                      importance = "impurity",
                                      num.trees = 15,
                                      plot = TRUE,
                        stepFactor = 1.25,
                        improve = 0.01,
                  trace = TRUE)
  ii<-ii+1
}

```

